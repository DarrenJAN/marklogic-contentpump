<html xmlns="http://www.w3.org/1999/xhtml">
<head><title>Overview of MarkLogic Connector for Hadoop</title</head>
<body>
<p>
  This bundle provides an API for a MarkLogic Server content
  connector for Apache Hadoop MapReduce. The overview covers the 
  following topics:
</p>
<ul>
  <li><a href="#Introduction">Introduction</a></li>
  <li><a href="#Configuration">Configuration</a></li>
  <li><a href="#Samples">Samples</a></li>
</ul>

<p>
  For detailed information, see the <em>Hadoop MapReduce MarkLogic Connector 
  for Hadoop Developer's Guide</em>.
</p>

<h2 id="Introduction">Introduction</h2>
<p>
  The MarkLogic Connector for Hadoop API allows you to
  use MarkLogic Server as either or both a Hadoop MapReduce
  input source and an output destination. 
</p>
<p>
  The following classes are provided for defining MarkLogic-specific
  key and value types for your MapReduce key-value pairs:
</p>
<ul>
  <li>{@link com.marklogic.mapreduce.NodePath} for keys</li>
  <li>{@link com.marklogic.mapreduce.DocumentURI} for keys</li>
  <li>{@link com.marklogic.mapreduce.MarkLogicNode} for values</li>
</ul>
<p>
  You may also use Apache Hadoop MapReduce types such as Text in
  certain circumstances. See {@link com.marklogic.mapreduce.ValueInputFormat}
  {@link com.marklogic.mapreduce.KeyValueInputFormat}.
</p>
<p>
  The following classes are provided for defining 
  MarkLogic-specific MapReduce input and output formats. 
  Input and output formats need not be the same type.
</p>
<ul>
  <li>{@link com.marklogic.mapreduce.DocumentInputFormat}</li>
  <li>{@link com.marklogic.mapreduce.NodeInputFormat}</li>
  <li>{@link com.marklogic.mapreduce.ValueInputFormat}</li>
  <li>{@link com.marklogic.mapreduce.KeyValueInputFormat}</li>
  <li>{@link com.marklogic.mapreduce.ContentOutputFormat}</li>
  <li>{@link com.marklogic.mapreduce.NodeOutputFormat}</li>
  <li>{@link com.marklogic.mapreduce.PropertyOutputFormat}</li>
</ul>

<h2 id="Configuration">Configuration</h2>
<p>
  Configure the connector using the standard Hadoop configuration
  mechanism. That is, use a Hadoop configuration file to define
  property values, or set properties programmatically on your
  Job's {@link org.apache.hadoop.conf.Configuration} object.
</p>
<p>
  The configuration properties available for the connector are
  described in {@link com.marklogic.mapreduce.MarkLogicConstants}.
</p>
<p>
 When using MarkLogic Server as an input source for MapReduce
 tasks, you may use either basic or advanced input mode. The default 
 is <code>basic</code> mode. The mode is controlled through
 the {@link com.marklogic.mapreduce.MarkLogicConstants#INPUT_MODE
 mapreduce.marklogic.input.mode} property. The following sections
 describe the input modes briefly. For details, see the
 <em>MarkLogic Connector for Hadoop Developer's Guide</em>.
</p>

<h3>Configuring the Input Query in Basic Mode</h3>
<p>
 In basic mode, you supply components of an XQuery path expression
 which the connector uses to generate input data. To allow MarkLogic 
 Server to optimize the input query, the path expression is  
 constructed from two components: A 
 {@link com.marklogic.mapreduce.MarkLogicConstants#DOCUMENT_SELECTOR 
 document node selector} and a
 {@link com.marklogic.mapreduce.MarkLogicConstants#SUBDOCUMENT_EXPRESSION
 sub-document expression}.
</p>
<p>
  The input split is not configurable in <code>basic</code> mode. The
  splits are based on a rough count of the number of fragments in
  each forest. Use <code>advanced</code> input mode for more control
  over input split generation.
</p>
<p>
 Conceptually, the input data for each task is constructed from a 
 path expression similar to:
</p>
<pre class="codesample"><code>
$document-selector/$subdocument-expression
</code></pre>
<p>
 Both components of the input path expression are optional. If no 
 document selector is given, <code>fn:collection()</code> is used.
 If no subdocument expression is given, the document nodes returned
 by the document selector are used as the input values.
</p>
<p>Examples:</p>
<pre class="codesample"><code>
document selector: none
subdocument expression: none
  =&gt; All document nodes in fn:collection()

document selector: fn:collection("wiki-topics")
subdocument expression: none
  =&gt; All document nodes in the "wiki-topics" collection

document selector: fn:collection("wiki-topics")
subdocument expression: //wp:a[@href]
  =&gt; All elements in the "wiki-topics" collection containing hrefs

document selector: fn:collection("wiki-topics")
subdocument expression: //wp:a[@href]/@title
  =&gt; The titles of all documents in the "wiki-topics" collection 
     containing hrefs
</code></pre>
 
<h3>Configuring the Input Query in Advanced Mode</h3>
<p>
 In <code>advanced</code> input mode, you must supply an 
 {@link com.marklogic.mapreduce.MarkLogicConstants#SPLIT_QUERY 
 input split query} and an 
 {@link com.marklogic.mapreduce.MarkLogicConstants#INPUT_QUERY
 input query}.
 </p>
 <p>
  The split query is used to generate meta-data for Hadoop's
  input splits. This query must return a sequence of triples, 
  each of which includes a forest id, record (fragment) count, 
  and list of host names. The count may be an estimate.
</p>
<p>
 The input query is used to fetch the input data for each map task.
 This query must return data that matches the configured InputFormat
 subclass.
</p>

<h2 id="Samples">Samples</h2>
<p>
   Sample code is included in {@link com.marklogic.mapreduce.examples},
   with corresponding example config files under the conf/ directory
   in the connector distribution. A brief overview of each sample is
   given below. For detailed instructions, see the 
   <em>MarkLogic Connector for Hadoop Developer's Guide</em>.
</p>
<table class="simple-table">
  <tr>
    <td>{@link com.marklogic.mapreduce.examples.LinkCountValue}</td>
    <td>
      Analyzes links between documents in a Wikipedia collection,
      using basic input mode with {@link com.marklogic.mapreduce.ValueInputFormat}
      and TextOutputFormat. See the <code>marklogic-textin-textout.xml</code> 
      config file.
    </td>
  </tr>
  <tr>
    <td>{@link com.marklogic.mapreduce.examples.LinkCountInDoc}</td>
    <td>
      Analyzes links between documents in a Wikipedia collection, using
      basic input mode with {@link com.marklogic.mapreduce.NodeInputFormat}
      and {@link com.marklogic.mapreduce.NodeOutputFormat}. Results are
      as a new text element on the documents. See the 
      <code>marklogic-nodein-nodeout.xml</code> config file.
    </td>
  </tr>
  <tr>
    <td>{@link com.marklogic.mapreduce.examples.LinkCountInProperty}</td>
    <td>Analyzes links between documents in a Wikipedia collection, using
       basic input mode with {@link com.marklogic.mapreduce.PropertyOutputFormat}.
       Results are stored in a document property. See the
      <code>marklogic-textin-propout.xml</code> config file.
    </td>
  </tr>
  <tr>
    <td>{@link com.marklogic.mapreduce.examples.LinkCount}</td>
    <td>Analyzes links between documents in a Wikipedia collection, using
       advanced input mode with {@link com.marklogic.mapreduce.ValueInputFormat}.
       Results are stored in HDFS. See the
      <code>marklogic-advanced.xml</code> config file.
    </td>
  </tr>
  <tr>
    <td>{@link com.marklogic.mapreduce.examples.ContentLoader}</td>
    <td>Loads text files from HDFS into MarkLogic Server documents.
      See the <code>marklogic-textin-docout.xml</code> config file.
    </td>      
  </tr>
  <tr>
    <td>{@link com.marklogic.mapreduce.examples.ContentReader}</td>
    <td>Copies documents from a MarkLogic Server database to HDFS
      text files using an SSL-enabled connection. See the 
      <code>marklogic-docin-textout.xml</code> config file.
    </td>      
  </tr>
  <tr>
    <td>{@link com.marklogic.mapreduce.examples.ZipContentLoader}</td>
    <td>Loads text files from a zip file in HDFS into MarkLogic
      Server documents. See the
      <code>marklogic-textin-docout.xml</code> config file.
    </td>      
  </tr>
</table>
</body>
</html>

